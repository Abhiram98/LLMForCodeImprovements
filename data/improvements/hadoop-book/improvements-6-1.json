{
    "Method_Improvements": {
        "Improvements": [
            {
                "Improvement": "Extract print usage into a separate method",
                "Change_Diff": "- System.out.println(\"Usage: writer <out-dir>\");\n- ToolRunner.printGenericCommandUsage(System.out);\n- return -1;\n+ return printUsage();",
                "Description": "The code that prints usage information seems to be a separate responsibility. It could be extracted into its own method to make the main method more readable. This follows the Single Responsibility Principle.",
                "Start": 5,
                "End": 8
            },
            {
                "Improvement": "Avoid magic numbers",
                "Change_Diff": "- int numMapsPerHost=job.getInt(\"test.randomwriter.maps_per_host\",10);\n- long numBytesToWritePerMap=job.getLong(\"test.randomwrite.bytes_per_map\",1 * 1024 * 1024* 1024);\n+ int numMapsPerHost=job.getInt(\"test.randomwriter.maps_per_host\",MAPS_PER_HOST);\n+ long numBytesToWritePerMap=job.getLong(\"test.randomwrite.bytes_per_map\",BYTES_PER_MAP);",
                "Description": "The code contains magic numbers (10 and 1 * 1024 * 1024* 1024). Use named constants instead to make the code more readable and maintainable.",
                "Start": 19,
                "End": 20
            },
            {
                "Improvement": "Use logger instead of System.out.println",
                "Change_Diff": "- System.out.println(...);\n+ LOGGER.info(...);",
                "Description": "Use a logger instead of System.out.println for printing messages. It gives you more flexibility (for example, control over verbosity levels, different targets, etc.) and is generally better practice.",
                "Start": 39,
                "End": 46
            }
        ],
        "Final code": "public int run(String[] args) throws Exception {\n  if (args.length == 0) {\n    return printUsage();\n  }\n  Path outDir=new Path(args[0]);\n  JobConf job=new JobConf(getConf());\n  job.setJarByClass(RandomWriter.class);\n  job.setJobName(\"random-writer\");\n  FileOutputFormat.setOutputPath(job,outDir);\n  job.setOutputKeyClass(BytesWritable.class);\n  job.setOutputValueClass(BytesWritable.class);\n  job.setInputFormat(RandomInputFormat.class);\n  job.setMapperClass(Map.class);\n  job.setReducerClass(IdentityReducer.class);\n  job.setOutputFormat(SequenceFileOutputFormat.class);\n  JobClient client=new JobClient(job);\n  ClusterStatus cluster=client.getClusterStatus();\n  int numMapsPerHost=job.getInt(\"test.randomwriter.maps_per_host\",MAPS_PER_HOST);\n  long numBytesToWritePerMap=job.getLong(\"test.randomwrite.bytes_per_map\",BYTES_PER_MAP);\n  if (numBytesToWritePerMap == 0) {\n    LOGGER.error(\"Cannot have test.randomwrite.bytes_per_map set to 0\");\n    return -2;\n  }\n  long totalBytesToWrite=job.getLong(\"test.randomwrite.total_bytes\",numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\n  int numMaps=(int)(totalBytesToWrite / numBytesToWritePerMap);\n  if (numMaps == 0 && totalBytesToWrite > 0) {\n    numMaps=1;\n    job.setLong(\"test.randomwrite.bytes_per_map\",totalBytesToWrite);\n  }\n  job.setNumMapTasks(numMaps);\n  LOGGER.info(\"Running \" + numMaps + \" maps.\");\n  job.setNumReduceTasks(0);\n  Date startTime=new Date();\n  LOGGER.info(\"Job started: \" + startTime);\n  JobClient.runJob(job);\n  Date endTime=new Date();\n  LOGGER.info(\"Job ended: \" + endTime);\n  LOGGER.info(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n\nprivate int printUsage(){\n  LOGGER.info(\"Usage: writer <out-dir>\");\n  ToolRunner.printGenericCommandUsage(System.out);\n  return -1;\n}"
    },
    "Old_Method": "/** \n * This is the main routine for launching a distributed random write job. It runs 10 maps/node and each node writes 1 gig of data to a DFS file. The reduce doesn't do anything.\n * @throws IOException\n */\npublic int run(String[] args) throws Exception {\n  if (args.length == 0) {\n    System.out.println(\"Usage: writer <out-dir>\");\n    ToolRunner.printGenericCommandUsage(System.out);\n    return -1;\n  }\n  Path outDir=new Path(args[0]);\n  JobConf job=new JobConf(getConf());\n  job.setJarByClass(RandomWriter.class);\n  job.setJobName(\"random-writer\");\n  FileOutputFormat.setOutputPath(job,outDir);\n  job.setOutputKeyClass(BytesWritable.class);\n  job.setOutputValueClass(BytesWritable.class);\n  job.setInputFormat(RandomInputFormat.class);\n  job.setMapperClass(Map.class);\n  job.setReducerClass(IdentityReducer.class);\n  job.setOutputFormat(SequenceFileOutputFormat.class);\n  JobClient client=new JobClient(job);\n  ClusterStatus cluster=client.getClusterStatus();\n  int numMapsPerHost=job.getInt(\"test.randomwriter.maps_per_host\",10);\n  long numBytesToWritePerMap=job.getLong(\"test.randomwrite.bytes_per_map\",1 * 1024 * 1024* 1024);\n  if (numBytesToWritePerMap == 0) {\n    System.err.println(\"Cannot have test.randomwrite.bytes_per_map set to 0\");\n    return -2;\n  }\n  long totalBytesToWrite=job.getLong(\"test.randomwrite.total_bytes\",numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\n  int numMaps=(int)(totalBytesToWrite / numBytesToWritePerMap);\n  if (numMaps == 0 && totalBytesToWrite > 0) {\n    numMaps=1;\n    job.setLong(\"test.randomwrite.bytes_per_map\",totalBytesToWrite);\n  }\n  job.setNumMapTasks(numMaps);\n  System.out.println(\"Running \" + numMaps + \" maps.\");\n  job.setNumReduceTasks(0);\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  JobClient.runJob(job);\n  Date endTime=new Date();\n  System.out.println(\"Job ended: \" + endTime);\n  System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
    "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/RandomWriter.java",
    "Start": 8237,
    "Stop": 10671,
    "Project_Name": "data/projects/hadoop-book",
    "Method_Name": "run"
}
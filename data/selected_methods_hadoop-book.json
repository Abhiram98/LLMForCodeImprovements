[
    {
        "Old_Method": "/** \n * The main driver for sort program. Invoke this method to submit the map/reduce job.\n * @throws IOException When there is communication problems with the job tracker.\n */\npublic int run(String[] args) throws Exception {\n  JobConf jobConf=new JobConf(getConf(),Sort.class);\n  jobConf.setJobName(\"sorter\");\n  jobConf.setMapperClass(IdentityMapper.class);\n  jobConf.setReducerClass(IdentityReducer.class);\n  JobClient client=new JobClient(jobConf);\n  ClusterStatus cluster=client.getClusterStatus();\n  int num_reduces=(int)(cluster.getMaxReduceTasks() * 0.9);\n  String sort_reduces=jobConf.get(\"test.sort.reduces_per_host\");\n  if (sort_reduces != null) {\n    num_reduces=cluster.getTaskTrackers() * Integer.parseInt(sort_reduces);\n  }\n  Class<? extends InputFormat> inputFormatClass=SequenceFileInputFormat.class;\n  Class<? extends OutputFormat> outputFormatClass=SequenceFileOutputFormat.class;\n  Class<? extends WritableComparable> outputKeyClass=BytesWritable.class;\n  Class<? extends Writable> outputValueClass=BytesWritable.class;\n  List<String> otherArgs=new ArrayList<String>();\n  InputSampler.Sampler<K,V> sampler=null;\n  for (int i=0; i < args.length; ++i) {\n    try {\n      if (\"-m\".equals(args[i])) {\n        jobConf.setNumMapTasks(Integer.parseInt(args[++i]));\n      }\n else       if (\"-r\".equals(args[i])) {\n        num_reduces=Integer.parseInt(args[++i]);\n      }\n else       if (\"-inFormat\".equals(args[i])) {\n        inputFormatClass=Class.forName(args[++i]).asSubclass(InputFormat.class);\n      }\n else       if (\"-outFormat\".equals(args[i])) {\n        outputFormatClass=Class.forName(args[++i]).asSubclass(OutputFormat.class);\n      }\n else       if (\"-outKey\".equals(args[i])) {\n        outputKeyClass=Class.forName(args[++i]).asSubclass(WritableComparable.class);\n      }\n else       if (\"-outValue\".equals(args[i])) {\n        outputValueClass=Class.forName(args[++i]).asSubclass(Writable.class);\n      }\n else       if (\"-totalOrder\".equals(args[i])) {\n        double pcnt=Double.parseDouble(args[++i]);\n        int numSamples=Integer.parseInt(args[++i]);\n        int maxSplits=Integer.parseInt(args[++i]);\n        if (0 >= maxSplits)         maxSplits=Integer.MAX_VALUE;\n        sampler=new InputSampler.RandomSampler<K,V>(pcnt,numSamples,maxSplits);\n      }\n else {\n        otherArgs.add(args[i]);\n      }\n    }\n catch (    NumberFormatException except) {\n      System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\n      return printUsage();\n    }\ncatch (    ArrayIndexOutOfBoundsException except) {\n      System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\n      return printUsage();\n    }\n  }\n  jobConf.setNumReduceTasks(num_reduces);\n  jobConf.setInputFormat(inputFormatClass);\n  jobConf.setOutputFormat(outputFormatClass);\n  jobConf.setOutputKeyClass(outputKeyClass);\n  jobConf.setOutputValueClass(outputValueClass);\n  if (otherArgs.size() != 2) {\n    System.out.println(\"ERROR: Wrong number of parameters: \" + otherArgs.size() + \" instead of 2.\");\n    return printUsage();\n  }\n  FileInputFormat.setInputPaths(jobConf,otherArgs.get(0));\n  FileOutputFormat.setOutputPath(jobConf,new Path(otherArgs.get(1)));\n  if (sampler != null) {\n    System.out.println(\"Sampling input to effect total-order sort...\");\n    jobConf.setPartitionerClass(TotalOrderPartitioner.class);\n    Path inputDir=FileInputFormat.getInputPaths(jobConf)[0];\n    inputDir=inputDir.makeQualified(inputDir.getFileSystem(jobConf));\n    Path partitionFile=new Path(inputDir,\"_sortPartitioning\");\n    TotalOrderPartitioner.setPartitionFile(jobConf,partitionFile);\n    InputSampler.<K,V>writePartitionFile(jobConf,sampler);\n    URI partitionUri=new URI(partitionFile.toString() + \"#\" + \"_sortPartitioning\");\n    DistributedCache.addCacheFile(partitionUri,jobConf);\n    DistributedCache.createSymlink(jobConf);\n  }\n  System.out.println(\"Running on \" + cluster.getTaskTrackers() + \" nodes to sort from \"+ FileInputFormat.getInputPaths(jobConf)[0]+ \" into \"+ FileOutputFormat.getOutputPath(jobConf)+ \" with \"+ num_reduces+ \" reduces.\");\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  jobResult=JobClient.runJob(jobConf);\n  Date end_time=new Date();\n  System.out.println(\"Job ended: \" + end_time);\n  System.out.println(\"The job took \" + (end_time.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/Sort.java",
        "Start": 2959,
        "Stop": 7891,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "run"
    },
    {
        "Old_Method": "/** \n * This is the main routine for launching a distributed random write job. It runs 10 maps/node and each node writes 1 gig of data to a DFS file. The reduce doesn't do anything.\n * @throws IOException\n */\npublic int run(String[] args) throws Exception {\n  if (args.length == 0) {\n    return printUsage();\n  }\n  JobConf job=new JobConf(getConf());\n  job.setJarByClass(RandomTextWriter.class);\n  job.setJobName(\"random-text-writer\");\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(Text.class);\n  job.setInputFormat(RandomWriter.RandomInputFormat.class);\n  job.setMapperClass(Map.class);\n  JobClient client=new JobClient(job);\n  ClusterStatus cluster=client.getClusterStatus();\n  int numMapsPerHost=job.getInt(\"test.randomtextwrite.maps_per_host\",10);\n  long numBytesToWritePerMap=job.getLong(\"test.randomtextwrite.bytes_per_map\",1 * 1024 * 1024* 1024);\n  if (numBytesToWritePerMap == 0) {\n    System.err.println(\"Cannot have test.randomtextwrite.bytes_per_map set to 0\");\n    return -2;\n  }\n  long totalBytesToWrite=job.getLong(\"test.randomtextwrite.total_bytes\",numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\n  int numMaps=(int)(totalBytesToWrite / numBytesToWritePerMap);\n  if (numMaps == 0 && totalBytesToWrite > 0) {\n    numMaps=1;\n    job.setLong(\"test.randomtextwrite.bytes_per_map\",totalBytesToWrite);\n  }\n  Class<? extends OutputFormat> outputFormatClass=SequenceFileOutputFormat.class;\n  List<String> otherArgs=new ArrayList<String>();\n  for (int i=0; i < args.length; ++i) {\n    try {\n      if (\"-outFormat\".equals(args[i])) {\n        outputFormatClass=Class.forName(args[++i]).asSubclass(OutputFormat.class);\n      }\n else {\n        otherArgs.add(args[i]);\n      }\n    }\n catch (    ArrayIndexOutOfBoundsException except) {\n      System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\n      return printUsage();\n    }\n  }\n  job.setOutputFormat(outputFormatClass);\n  FileOutputFormat.setOutputPath(job,new Path(otherArgs.get(0)));\n  job.setNumMapTasks(numMaps);\n  System.out.println(\"Running \" + numMaps + \" maps.\");\n  job.setNumReduceTasks(0);\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  JobClient.runJob(job);\n  Date endTime=new Date();\n  System.out.println(\"Job ended: \" + endTime);\n  System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/RandomTextWriter.java",
        "Start": 6049,
        "Stop": 9055,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "run"
    },
    {
        "Old_Method": "/** \n * This is the main routine for launching a distributed random write job. It runs 10 maps/node and each node writes 1 gig of data to a DFS file. The reduce doesn't do anything.\n * @throws IOException\n */\npublic int run(String[] args) throws Exception {\n  if (args.length == 0) {\n    return printUsage();\n  }\n  JobConf job=new JobConf(getConf());\n  job.setJarByClass(RandomTextWriter.class);\n  job.setJobName(\"random-text-writer\");\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(Text.class);\n  job.setInputFormat(RandomWriter.RandomInputFormat.class);\n  job.setMapperClass(Map.class);\n  JobClient client=new JobClient(job);\n  ClusterStatus cluster=client.getClusterStatus();\n  int numMapsPerHost=job.getInt(\"test.randomtextwrite.maps_per_host\",10);\n  long numBytesToWritePerMap=job.getLong(\"test.randomtextwrite.bytes_per_map\",1 * 1024 * 1024* 1024);\n  if (numBytesToWritePerMap == 0) {\n    System.err.println(\"Cannot have test.randomtextwrite.bytes_per_map set to 0\");\n    return -2;\n  }\n  long totalBytesToWrite=job.getLong(\"test.randomtextwrite.total_bytes\",numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\n  int numMaps=(int)(totalBytesToWrite / numBytesToWritePerMap);\n  if (numMaps == 0 && totalBytesToWrite > 0) {\n    numMaps=1;\n    job.setLong(\"test.randomtextwrite.bytes_per_map\",totalBytesToWrite);\n  }\n  Class<? extends OutputFormat> outputFormatClass=SequenceFileOutputFormat.class;\n  List<String> otherArgs=new ArrayList<String>();\n  for (int i=0; i < args.length; ++i) {\n    try {\n      if (\"-outFormat\".equals(args[i])) {\n        outputFormatClass=Class.forName(args[++i]).asSubclass(OutputFormat.class);\n      }\n else {\n        otherArgs.add(args[i]);\n      }\n    }\n catch (    ArrayIndexOutOfBoundsException except) {\n      System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\n      return printUsage();\n    }\n  }\n  job.setOutputFormat(outputFormatClass);\n  FileOutputFormat.setOutputPath(job,new Path(otherArgs.get(0)));\n  job.setNumMapTasks(numMaps);\n  System.out.println(\"Running \" + numMaps + \" maps.\");\n  job.setNumReduceTasks(0);\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  JobClient.runJob(job);\n  Date endTime=new Date();\n  System.out.println(\"Job ended: \" + endTime);\n  System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/RandomTextWriter.java",
        "Start": 6049,
        "Stop": 9055,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "run"
    },
    {
        "Old_Method": "/** \n * Add a row to the table.\n * @param values the columns that are satisfied by this row\n */\npublic void addRow(boolean[] values){\n  Node<ColumnName> prev=null;\n  for (int i=0; i < values.length; ++i) {\n    if (values[i]) {\n      ColumnHeader<ColumnName> top=columns.get(i);\n      top.size+=1;\n      Node<ColumnName> bottom=top.up;\n      Node<ColumnName> node=new Node<ColumnName>(null,null,bottom,top,top);\n      bottom.down=node;\n      top.up=node;\n      if (prev != null) {\n        Node<ColumnName> front=prev.right;\n        node.left=prev;\n        node.right=front;\n        prev.right=node;\n        front.left=node;\n      }\n else {\n        node.left=node;\n        node.right=node;\n      }\n      prev=node;\n    }\n  }\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/dancing/DancingLinks.java",
        "Start": 5022,
        "Stop": 6050,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "addRow"
    },
    {
        "Old_Method": "/** \n * Add a row to the table.\n * @param values the columns that are satisfied by this row\n */\npublic void addRow(boolean[] values){\n  Node<ColumnName> prev=null;\n  for (int i=0; i < values.length; ++i) {\n    if (values[i]) {\n      ColumnHeader<ColumnName> top=columns.get(i);\n      top.size+=1;\n      Node<ColumnName> bottom=top.up;\n      Node<ColumnName> node=new Node<ColumnName>(null,null,bottom,top,top);\n      bottom.down=node;\n      top.up=node;\n      if (prev != null) {\n        Node<ColumnName> front=prev.right;\n        node.left=prev;\n        node.right=front;\n        prev.right=node;\n        front.left=node;\n      }\n else {\n        node.left=node;\n        node.right=node;\n      }\n      prev=node;\n    }\n  }\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/dancing/DancingLinks.java",
        "Start": 5022,
        "Stop": 6050,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "addRow"
    },
    {
        "Old_Method": "/** \n * The main driver for sort program. Invoke this method to submit the map/reduce job.\n * @throws IOException When there is communication problems with the jobtracker.\n */\n@Override public int run(String[] args) throws Exception {\n  JobConf jobConf=new JobConf(getConf(),Sort.class);\n  jobConf.setJobName(\"join\");\n  jobConf.setMapperClass(IdentityMapper.class);\n  jobConf.setReducerClass(IdentityReducer.class);\n  JobClient client=new JobClient(jobConf);\n  ClusterStatus cluster=client.getClusterStatus();\n  int num_maps=cluster.getTaskTrackers() * jobConf.getInt(\"test.sort.maps_per_host\",10);\n  int num_reduces=(int)(cluster.getMaxReduceTasks() * 0.9);\n  String sort_reduces=jobConf.get(\"test.sort.reduces_per_host\");\n  if (sort_reduces != null) {\n    num_reduces=cluster.getTaskTrackers() * Integer.parseInt(sort_reduces);\n  }\n  Class<? extends InputFormat> inputFormatClass=SequenceFileInputFormat.class;\n  Class<? extends OutputFormat> outputFormatClass=SequenceFileOutputFormat.class;\n  Class<? extends WritableComparable> outputKeyClass=BytesWritable.class;\n  Class<? extends Writable> outputValueClass=TupleWritable.class;\n  String op=\"inner\";\n  List<String> otherArgs=new ArrayList<String>();\n  for (int i=0; i < args.length; ++i) {\n    try {\n      if (\"-m\".equals(args[i])) {\n        num_maps=Integer.parseInt(args[++i]);\n      }\n else       if (\"-r\".equals(args[i])) {\n        num_reduces=Integer.parseInt(args[++i]);\n      }\n else       if (\"-inFormat\".equals(args[i])) {\n        inputFormatClass=Class.forName(args[++i]).asSubclass(InputFormat.class);\n      }\n else       if (\"-outFormat\".equals(args[i])) {\n        outputFormatClass=Class.forName(args[++i]).asSubclass(OutputFormat.class);\n      }\n else       if (\"-outKey\".equals(args[i])) {\n        outputKeyClass=Class.forName(args[++i]).asSubclass(WritableComparable.class);\n      }\n else       if (\"-outValue\".equals(args[i])) {\n        outputValueClass=Class.forName(args[++i]).asSubclass(Writable.class);\n      }\n else       if (\"-joinOp\".equals(args[i])) {\n        op=args[++i];\n      }\n else {\n        otherArgs.add(args[i]);\n      }\n    }\n catch (    NumberFormatException except) {\n      System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\n      return printUsage();\n    }\ncatch (    ArrayIndexOutOfBoundsException except) {\n      System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\n      return printUsage();\n    }\n  }\n  jobConf.setNumMapTasks(num_maps);\n  jobConf.setNumReduceTasks(num_reduces);\n  if (otherArgs.size() < 2) {\n    System.out.println(\"ERROR: Wrong number of parameters: \");\n    return printUsage();\n  }\n  FileOutputFormat.setOutputPath(jobConf,new Path(otherArgs.remove(otherArgs.size() - 1)));\n  List<Path> plist=new ArrayList<Path>(otherArgs.size());\n  for (  String s : otherArgs) {\n    plist.add(new Path(s));\n  }\n  jobConf.setInputFormat(CompositeInputFormat.class);\n  jobConf.set(\"mapred.join.expr\",CompositeInputFormat.compose(op,inputFormatClass,plist.toArray(new Path[0])));\n  jobConf.setOutputFormat(outputFormatClass);\n  jobConf.setOutputKeyClass(outputKeyClass);\n  jobConf.setOutputValueClass(outputValueClass);\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  JobClient.runJob(jobConf);\n  Date end_time=new Date();\n  System.out.println(\"Job ended: \" + end_time);\n  System.out.println(\"The job took \" + (end_time.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/Join.java",
        "Start": 2701,
        "Stop": 7066,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "run"
    },
    {
        "Old_Method": "/** \n * This is the main routine for launching a distributed random write job. It runs 10 maps/node and each node writes 1 gig of data to a DFS file. The reduce doesn't do anything.\n * @throws IOException\n */\npublic int run(String[] args) throws Exception {\n  if (args.length == 0) {\n    System.out.println(\"Usage: writer <out-dir>\");\n    ToolRunner.printGenericCommandUsage(System.out);\n    return -1;\n  }\n  Path outDir=new Path(args[0]);\n  JobConf job=new JobConf(getConf());\n  job.setJarByClass(RandomWriter.class);\n  job.setJobName(\"random-writer\");\n  FileOutputFormat.setOutputPath(job,outDir);\n  job.setOutputKeyClass(BytesWritable.class);\n  job.setOutputValueClass(BytesWritable.class);\n  job.setInputFormat(RandomInputFormat.class);\n  job.setMapperClass(Map.class);\n  job.setReducerClass(IdentityReducer.class);\n  job.setOutputFormat(SequenceFileOutputFormat.class);\n  JobClient client=new JobClient(job);\n  ClusterStatus cluster=client.getClusterStatus();\n  int numMapsPerHost=job.getInt(\"test.randomwriter.maps_per_host\",10);\n  long numBytesToWritePerMap=job.getLong(\"test.randomwrite.bytes_per_map\",1 * 1024 * 1024* 1024);\n  if (numBytesToWritePerMap == 0) {\n    System.err.println(\"Cannot have test.randomwrite.bytes_per_map set to 0\");\n    return -2;\n  }\n  long totalBytesToWrite=job.getLong(\"test.randomwrite.total_bytes\",numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\n  int numMaps=(int)(totalBytesToWrite / numBytesToWritePerMap);\n  if (numMaps == 0 && totalBytesToWrite > 0) {\n    numMaps=1;\n    job.setLong(\"test.randomwrite.bytes_per_map\",totalBytesToWrite);\n  }\n  job.setNumMapTasks(numMaps);\n  System.out.println(\"Running \" + numMaps + \" maps.\");\n  job.setNumReduceTasks(0);\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  JobClient.runJob(job);\n  Date endTime=new Date();\n  System.out.println(\"Job ended: \" + endTime);\n  System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/RandomWriter.java",
        "Start": 8237,
        "Stop": 10671,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "run"
    },
    {
        "Old_Method": "/** \n * This is the main routine for launching a distributed random write job. It runs 10 maps/node and each node writes 1 gig of data to a DFS file. The reduce doesn't do anything.\n * @throws IOException\n */\npublic int run(String[] args) throws Exception {\n  if (args.length == 0) {\n    System.out.println(\"Usage: writer <out-dir>\");\n    ToolRunner.printGenericCommandUsage(System.out);\n    return -1;\n  }\n  Path outDir=new Path(args[0]);\n  JobConf job=new JobConf(getConf());\n  job.setJarByClass(RandomWriter.class);\n  job.setJobName(\"random-writer\");\n  FileOutputFormat.setOutputPath(job,outDir);\n  job.setOutputKeyClass(BytesWritable.class);\n  job.setOutputValueClass(BytesWritable.class);\n  job.setInputFormat(RandomInputFormat.class);\n  job.setMapperClass(Map.class);\n  job.setReducerClass(IdentityReducer.class);\n  job.setOutputFormat(SequenceFileOutputFormat.class);\n  JobClient client=new JobClient(job);\n  ClusterStatus cluster=client.getClusterStatus();\n  int numMapsPerHost=job.getInt(\"test.randomwriter.maps_per_host\",10);\n  long numBytesToWritePerMap=job.getLong(\"test.randomwrite.bytes_per_map\",1 * 1024 * 1024* 1024);\n  if (numBytesToWritePerMap == 0) {\n    System.err.println(\"Cannot have test.randomwrite.bytes_per_map set to 0\");\n    return -2;\n  }\n  long totalBytesToWrite=job.getLong(\"test.randomwrite.total_bytes\",numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\n  int numMaps=(int)(totalBytesToWrite / numBytesToWritePerMap);\n  if (numMaps == 0 && totalBytesToWrite > 0) {\n    numMaps=1;\n    job.setLong(\"test.randomwrite.bytes_per_map\",totalBytesToWrite);\n  }\n  job.setNumMapTasks(numMaps);\n  System.out.println(\"Running \" + numMaps + \" maps.\");\n  job.setNumReduceTasks(0);\n  Date startTime=new Date();\n  System.out.println(\"Job started: \" + startTime);\n  JobClient.runJob(job);\n  Date endTime=new Date();\n  System.out.println(\"Job ended: \" + endTime);\n  System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\n  return 0;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/RandomWriter.java",
        "Start": 8237,
        "Stop": 10671,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "run"
    },
    {
        "Old_Method": "/** \n * Create the desired number of splits, dividing the number of rows between the mappers.\n */\npublic InputSplit[] getSplits(JobConf job,int numSplits){\n  long totalRows=getNumberOfRows(job);\n  long rowsPerSplit=totalRows / numSplits;\n  System.out.println(\"Generating \" + totalRows + \" using \"+ numSplits+ \" maps with step of \"+ rowsPerSplit);\n  InputSplit[] splits=new InputSplit[numSplits];\n  long currentRow=0;\n  for (int split=0; split < numSplits - 1; ++split) {\n    splits[split]=new RangeInputSplit(currentRow,rowsPerSplit);\n    currentRow+=rowsPerSplit;\n  }\n  splits[numSplits - 1]=new RangeInputSplit(currentRow,totalRows - currentRow);\n  return splits;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/terasort/TeraGen.java",
        "Start": 4921,
        "Stop": 5780,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "getSplits"
    },
    {
        "Old_Method": "private DancingLinks<ColumnName> makeModel(){\n  DancingLinks<ColumnName> model=new DancingLinks<ColumnName>();\n  for (int x=0; x < size; ++x) {\n    for (int num=1; num <= size; ++num) {\n      model.addColumn(new ColumnConstraint(num,x));\n    }\n  }\n  for (int y=0; y < size; ++y) {\n    for (int num=1; num <= size; ++num) {\n      model.addColumn(new RowConstraint(num,y));\n    }\n  }\n  for (int x=0; x < squareYSize; ++x) {\n    for (int y=0; y < squareXSize; ++y) {\n      for (int num=1; num <= size; ++num) {\n        model.addColumn(new SquareConstraint(num,x,y));\n      }\n    }\n  }\n  for (int x=0; x < size; ++x) {\n    for (int y=0; y < size; ++y) {\n      model.addColumn(new CellConstraint(x,y));\n    }\n  }\n  boolean[] rowValues=new boolean[size * size * 4];\n  for (int x=0; x < size; ++x) {\n    for (int y=0; y < size; ++y) {\n      if (board[y][x] == -1) {\n        for (int num=1; num <= size; ++num) {\n          model.addRow(generateRow(rowValues,x,y,num));\n        }\n      }\n else {\n        model.addRow(generateRow(rowValues,x,y,board[y][x]));\n      }\n    }\n  }\n  return model;\n}\n",
        "File_Path": "hadoop-book/src/main/java/com/hadoopilluminated/examples/dancing/Sudoku.java",
        "Start": 8366,
        "Stop": 10077,
        "Project_Name": "data/projects/hadoop-book",
        "Method_Name": "makeModel"
    }
]